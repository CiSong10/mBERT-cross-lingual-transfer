{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertForQuestionAnswering, AdamW, BertTokenizerFast, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = load_dataset('squad_v2', split='train')\n",
    "squad_val = load_dataset('squad_v2', split='validation')\n",
    "mlqa_en = load_dataset(\"mlqa\", \"mlqa.en.en\", split=\"test\")\n",
    "mlqa_zh = load_dataset(\"mlqa\", \"mlqa.zh.zh\", split=\"test\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end(example):\n",
    "\n",
    "    if (len(example['answers']['text']) != 0):\n",
    "        context = example['context']\n",
    "        text = example['answers']['text'][0]\n",
    "        start_idx = example['answers']['answer_start'][0]\n",
    "\n",
    "        end_idx = start_idx + len(text)\n",
    "        \n",
    "        temp = example['answers'] # to change the value\n",
    "        temp['answer_end']=end_idx \n",
    "        temp['answer_start'] = start_idx # [num]->num\n",
    "        temp['text'] = text # ['text']->text\n",
    "    \n",
    "    else:\n",
    "        temp = example['answers']\n",
    "        temp['answer_end'] = 0 # []->0\n",
    "        temp['answer_start'] = 0 # []->0\n",
    "        temp['text'] = \"\" # []->\"\"\n",
    "        \n",
    "    return example\n",
    "\n",
    "squad_train = squad_train.map(find_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'answer_end': 286,\n",
       "  'answer_start': 269,\n",
       "  'text': 'in the late 1990s'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5a7e05ef70df9f001a875425',\n",
       " 'title': 'Matter',\n",
       " 'context': 'These quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.',\n",
       " 'question': 'How many quarks and leptons are there?',\n",
       " 'answers': {'answer_end': 0, 'answer_start': 0, 'text': ''}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokenized_train = tokenizer(squad_train['context'], squad_train['question'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_indexes(tokenized, dataset):\n",
    "    start_token_list = []\n",
    "    end_token_list = []\n",
    "    answers = dataset['answers']\n",
    "    for i in range(len(answers)):\n",
    "        if (answers[i]['text'] != ''):\n",
    "            start_token = tokenized.char_to_token(i, answers[i]['answer_start'])\n",
    "            end_token = tokenized.char_to_token(i, answers[i]['answer_end'] - 1)\n",
    "            \n",
    "            # if start token is None, the answer passage has been truncated\n",
    "            if start_token is None:\n",
    "                start_token = tokenizer.model_max_length\n",
    "            if end_token is None:\n",
    "                end_token = tokenizer.model_max_length\n",
    "        else:\n",
    "            start_token = 0\n",
    "            end_token = 0\n",
    "            \n",
    "        start_token_list.append(start_token)\n",
    "        end_token_list.append(end_token)\n",
    "\n",
    "    return start_token_list, start_token_list\n",
    "    \n",
    "s, e = find_token_indexes(tokenized_train, squad_train)\n",
    "squad_train = squad_train.add_column(\"start_position\", s)\n",
    "squad_train = squad_train.add_column(\"end_position\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers', 'start_position', 'end_position'],\n",
       "    num_rows: 130319\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_data = TensorDataset(torch.tensor(tokenized_train['input_ids'], dtype=torch.int64), \n",
    "                           torch.tensor(tokenized_train['token_type_ids'], dtype=torch.int64), \n",
    "                           torch.tensor(tokenized_train['attention_mask'], dtype=torch.float), \n",
    "                           torch.tensor(squad_train['start_position'], dtype=torch.int64), \n",
    "                           torch.tensor(squad_train['start_position'], dtype=torch.int64))\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/sw/pkgs/arc/python3.10-anaconda/2023.03/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "epochs = 3\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 16290/16290 [1:02:05<00:00,  4.37it/s, Loss=1.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.4995357166955154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 16290/16290 [1:02:21<00:00,  4.35it/s, Loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1804199021019184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 16290/16290 [1:01:54<00:00,  4.39it/s, Loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.0265117677540374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    count=-1\n",
    "    progress_bar = tqdm(train_dataloader, leave=True, position=0)\n",
    "    progress_bar.set_description(f\"Epoch {epoch+1}\")\n",
    "    for batch in progress_bar:\n",
    "        count+=1\n",
    "        input_ids, segment_ids, mask, start, end  = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss, start_logits, end_logits = model(input_ids = input_ids, \n",
    "                                                token_type_ids = segment_ids, \n",
    "                                                attention_mask = mask, \n",
    "                                                start_positions = start, \n",
    "                                                end_positions = end,\n",
    "                                                return_dict = False)           \n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (count % 20 == 0 and count != 0):\n",
    "            avg = total_loss/count\n",
    "            progress_bar.set_postfix(Loss=avg)\n",
    "            \n",
    "    torch.save(model.state_dict(), \"./bert_\" + str(epoch) + \".h5\") # save for later use\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    epoch_loss.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch} Loss: {avg_train_loss}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"./bert_2.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def f1_score(prediction, truth):\n",
    "    pred_tokens = prediction.split()\n",
    "    truth_tokens = truth.split()\n",
    "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, truth):\n",
    "    return prediction == truth\n",
    "\n",
    "def evaluate(validation_dataset):\n",
    "    # preprocess\n",
    "    tokenized_validation = tokenizer(validation_dataset['context'], \n",
    "                                     validation_dataset['question'], \n",
    "                                     truncation=True, \n",
    "                                     padding=True, \n",
    "                                     return_offsets_mapping=True)\n",
    "\n",
    "    batch_size = 8\n",
    "    val_data = TensorDataset(torch.tensor(tokenized_validation['input_ids'], dtype=torch.int64), \n",
    "                            torch.tensor(tokenized_validation['token_type_ids'], dtype=torch.int64), \n",
    "                            torch.tensor(tokenized_validation['attention_mask'], dtype=torch.float))\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # evaluate\n",
    "    threshold = 1.0\n",
    "\n",
    "    model.eval()\n",
    "    total_f1 = 0\n",
    "    total_exact_match = 0\n",
    "    num_evaluated = 0\n",
    "\n",
    "    for test_batch in tqdm(val_dataloader):\n",
    "        input_ids, segment_ids, masks = tuple(t.to(device) for t in test_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # prediction logits\n",
    "            start_logits, end_logits = model(input_ids=input_ids,\n",
    "                                            token_type_ids=segment_ids,\n",
    "                                            attention_mask=masks,\n",
    "                                            return_dict=False)\n",
    "\n",
    "        # to cpu\n",
    "        start_logits = start_logits.detach().cpu()\n",
    "        end_logits = end_logits.detach().cpu()\n",
    "\n",
    "        # for every sequence in batch \n",
    "        for bidx in range(len(start_logits)):\n",
    "            # apply softmax to logits to get scores\n",
    "            start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n",
    "            end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n",
    "\n",
    "            # find max for start<=end\n",
    "            size = len(start_scores)\n",
    "            scores = np.zeros((size, size))\n",
    "\n",
    "            for j in range(size):\n",
    "                for i in range(j+1): # include j\n",
    "                    scores[i,j] = start_scores[i] + end_scores[j]\n",
    "\n",
    "            # find best i and j\n",
    "            start_pred, end_pred = np.unravel_index(scores.argmax(), scores.shape)\n",
    "            answer_pred = \"\"\n",
    "            if scores[start_pred, end_pred] > threshold:\n",
    "                offsets = tokenized_validation.offset_mapping[num_evaluated]\n",
    "                pred_char_start = offsets[start_pred][0]\n",
    "\n",
    "                if end_pred < len(offsets):\n",
    "                    pred_char_end = offsets[end_pred][1]\n",
    "                    answer_pred = validation_dataset[num_evaluated]['context'][pred_char_start:pred_char_end]\n",
    "                else:\n",
    "                    answer_pred = validation_dataset[num_evaluated]['context'][pred_char_start:]\n",
    "\n",
    "            ground_truths = validation_dataset[num_evaluated]['answers']['text']\n",
    "            if ground_truths:\n",
    "                best_f1 = max(f1_score(answer_pred, truth) for truth in ground_truths)\n",
    "                total_f1 += best_f1\n",
    "\n",
    "                exact_match = any(exact_match_score(answer_pred, truth) for truth in ground_truths)\n",
    "                total_exact_match += int(exact_match)\n",
    "\n",
    "            num_evaluated += 1\n",
    "\n",
    "    avg_f1 = total_f1 / num_evaluated\n",
    "    avg_exact_match = total_exact_match / num_evaluated\n",
    "    print(\"Average F1 Score: \", avg_f1)\n",
    "    print(\"Exact Match Score: \", avg_exact_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1485/1485 [09:22<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score:  0.11357357518658164\n",
      "Exact Match Score:  0.06333698307083298\n"
     ]
    }
   ],
   "source": [
    "evaluate(squad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [09:12<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score:  0.2040851551647495\n",
      "Exact Match Score:  0.12174288179465056\n"
     ]
    }
   ],
   "source": [
    "evaluate(mlqa_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643/643 [04:02<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score:  0.00546084892979968\n",
      "Exact Match Score:  0.004477321393809616\n"
     ]
    }
   ],
   "source": [
    "evaluate(mlqa_zh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
